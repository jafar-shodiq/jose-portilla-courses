{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From the documentation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: (692,[244,263,272,300,301,328,350,351,378,379,405,406,407,428,433,434,455,456,461,462,483,484,489,490,496,511,512,517,539,540,568],[-7.353983524188197E-5,-9.102738505589466E-5,-1.9467430546904298E-4,-2.0300642473486668E-4,-3.1476183314863995E-5,-6.842977602660743E-5,1.5883626898239883E-5,1.4023497091372047E-5,3.5432047524968605E-4,1.1443272898171087E-4,1.0016712383666666E-4,6.014109303795481E-4,2.840248179122762E-4,-1.1541084736508837E-4,3.85996886312906E-4,6.35019557424107E-4,-1.1506412384575676E-4,-1.5271865864986808E-4,2.804933808994214E-4,6.070117471191634E-4,-2.008459663247437E-4,-1.421075579290126E-4,2.739010341160883E-4,2.7730456244968115E-4,-9.838027027269332E-5,-3.808522443517704E-4,-2.5315198008555033E-4,2.7747714770754307E-4,-2.443619763919199E-4,-0.0015394744687597765,-2.3073328411331293E-4]) Intercept: 0.22456315961250325\n",
      "Multinomial coefficients: 2 x 692 CSCMatrix\n",
      "(0,244) 4.290365458958277E-5\n",
      "(1,244) -4.290365458958294E-5\n",
      "(0,263) 6.488313287833108E-5\n",
      "(1,263) -6.488313287833092E-5\n",
      "(0,272) 1.2140666790834663E-4\n",
      "(1,272) -1.2140666790834657E-4\n",
      "(0,300) 1.3231861518665612E-4\n",
      "(1,300) -1.3231861518665607E-4\n",
      "(0,350) -6.775444746760509E-7\n",
      "(1,350) 6.775444746761932E-7\n",
      "(0,351) -4.899237909429297E-7\n",
      "(1,351) 4.899237909430322E-7\n",
      "(0,378) -3.5812102770679596E-5\n",
      "(1,378) 3.581210277067968E-5\n",
      "(0,379) -2.3539704331222065E-5\n",
      "(1,379) 2.353970433122204E-5\n",
      "(0,405) -1.90295199030314E-5\n",
      "(1,405) 1.90295199030314E-5\n",
      "(0,406) -5.626696935778909E-4\n",
      "(1,406) 5.626696935778912E-4\n",
      "(0,407) -5.121519619099504E-5\n",
      "(1,407) 5.1215196190995074E-5\n",
      "(0,428) 8.080614545413342E-5\n",
      "(1,428) -8.080614545413331E-5\n",
      "(0,433) -4.256734915330487E-5\n",
      "(1,433) 4.256734915330495E-5\n",
      "(0,434) -7.080191510151425E-4\n",
      "(1,434) 7.080191510151435E-4\n",
      "(0,455) 8.094482475733589E-5\n",
      "(1,455) -8.094482475733582E-5\n",
      "(0,456) 1.0433687128309833E-4\n",
      "(1,456) -1.0433687128309814E-4\n",
      "(0,461) -5.4466605046259246E-5\n",
      "(1,461) 5.4466605046259286E-5\n",
      "(0,462) -5.667133061990392E-4\n",
      "(1,462) 5.667133061990392E-4\n",
      "(0,483) 1.2495896045528374E-4\n",
      "(1,483) -1.249589604552838E-4\n",
      "(0,484) 9.810519424784944E-5\n",
      "(1,484) -9.810519424784941E-5\n",
      "(0,489) -4.88440907254626E-5\n",
      "(1,489) 4.8844090725462606E-5\n",
      "(0,490) -4.324392733454803E-5\n",
      "(1,490) 4.324392733454811E-5\n",
      "(0,496) 6.903351855620161E-5\n",
      "(1,496) -6.90335185562012E-5\n",
      "(0,511) 3.946505594172827E-4\n",
      "(1,511) -3.946505594172831E-4\n",
      "(0,512) 2.621745995919226E-4\n",
      "(1,512) -2.621745995919226E-4\n",
      "(0,517) -4.459475951170906E-5\n",
      "(1,517) 4.459475951170901E-5\n",
      "(0,539) 2.5417562428184555E-4\n",
      "(1,539) -2.5417562428184555E-4\n",
      "(0,540) 5.271781246228031E-4\n",
      "(1,540) -5.271781246228032E-4\n",
      "(0,568) 1.860255150352447E-4\n",
      "(1,568) -1.8602551503524485E-4\n",
      "Multinomial intercepts: [-0.12065879445860686,0.12065879445860686]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.classification.LogisticRegression\r\n",
       "training: org.apache.spark.sql.DataFrame = [label: double, features: vector]\r\n",
       "lr: org.apache.spark.ml.classification.LogisticRegression = logreg_cc65aa20ed1d\r\n",
       "lrModel: org.apache.spark.ml.classification.LogisticRegressionModel = LogisticRegressionModel: uid=logreg_cc65aa20ed1d, numClasses=2, numFeatures=692\r\n",
       "mlr: org.apache.spark.ml.classification.LogisticRegression = logreg_705b41b798d7\r\n",
       "mlrModel: org.apache.spark.ml.classification.LogisticRegressionModel = LogisticRegressionModel: uid=logreg_705b41b798d7, numClasses=2, numFeatures=692\r\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "\n",
    "// Load training data\n",
    "val training = spark.read.format(\"libsvm\").load(\"./datasets/sample_libsvm_data.txt\")\n",
    "\n",
    "val lr = new LogisticRegression()\n",
    "  .setMaxIter(10)\n",
    "  .setRegParam(0.3)\n",
    "  .setElasticNetParam(0.8)\n",
    "\n",
    "// Fit the model\n",
    "val lrModel = lr.fit(training)\n",
    "\n",
    "// Print the coefficients and intercept for logistic regression\n",
    "println(s\"Coefficients: ${lrModel.coefficients} Intercept: ${lrModel.intercept}\")\n",
    "\n",
    "// We can also use the multinomial family for binary classification\n",
    "val mlr = new LogisticRegression()\n",
    "  .setMaxIter(10)\n",
    "  .setRegParam(0.3)\n",
    "  .setElasticNetParam(0.8)\n",
    "  .setFamily(\"multinomial\")\n",
    "\n",
    "val mlrModel = mlr.fit(training)\n",
    "\n",
    "// Print the coefficients and intercepts for logistic regression with multinomial family\n",
    "println(s\"Multinomial coefficients: ${mlrModel.coefficientMatrix}\")\n",
    "println(s\"Multinomial intercepts: ${mlrModel.interceptVector}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Real dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\r\n",
       "import org.apache.spark.ml.classification.LogisticRegression\r\n",
       "import org.apache.log4j._\r\n",
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@701ece27\r\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "import org.apache.log4j._\n",
    "\n",
    "Logger.getLogger(\"org\").setLevel(Level.ERROR)\n",
    "val spark = SparkSession.builder().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25| null|       S|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925| null|       S|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|   53.1| C123|       S|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|   8.05| null|       S|\n",
      "|          6|       0|     3|    Moran, Mr. James|  male|null|    0|    0|          330877| 8.4583| null|       Q|\n",
      "|          7|       0|     1|McCarthy, Mr. Tim...|  male|54.0|    0|    0|           17463|51.8625|  E46|       S|\n",
      "|          8|       0|     3|Palsson, Master. ...|  male| 2.0|    3|    1|          349909| 21.075| null|       S|\n",
      "|          9|       1|     3|Johnson, Mrs. Osc...|female|27.0|    0|    2|          347742|11.1333| null|       S|\n",
      "|         10|       1|     2|Nasser, Mrs. Nich...|female|14.0|    1|    0|          237736|30.0708| null|       C|\n",
      "|         11|       1|     3|Sandstrom, Miss. ...|female| 4.0|    1|    1|         PP 9549|   16.7|   G6|       S|\n",
      "|         12|       1|     1|Bonnell, Miss. El...|female|58.0|    0|    0|          113783|  26.55| C103|       S|\n",
      "|         13|       0|     3|Saundercock, Mr. ...|  male|20.0|    0|    0|       A/5. 2151|   8.05| null|       S|\n",
      "|         14|       0|     3|Andersson, Mr. An...|  male|39.0|    1|    5|          347082| 31.275| null|       S|\n",
      "|         15|       0|     3|Vestrom, Miss. Hu...|female|14.0|    0|    0|          350406| 7.8542| null|       S|\n",
      "|         16|       1|     2|Hewlett, Mrs. (Ma...|female|55.0|    0|    0|          248706|   16.0| null|       S|\n",
      "|         17|       0|     3|Rice, Master. Eugene|  male| 2.0|    4|    1|          382652| 29.125| null|       Q|\n",
      "|         18|       1|     2|Williams, Mr. Cha...|  male|null|    0|    0|          244373|   13.0| null|       S|\n",
      "|         19|       0|     3|Vander Planke, Mr...|female|31.0|    1|    0|          345763|   18.0| null|       S|\n",
      "|         20|       1|     3|Masselmani, Mrs. ...|female|null|    0|    0|            2649|  7.225| null|       C|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "data: org.apache.spark.sql.DataFrame = [PassengerId: int, Survived: int ... 10 more fields]\r\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data = spark.read.option(\"header\", \"true\")\n",
    "                .option(\"inferSchema\", \"true\")\n",
    "                .format(\"csv\")\n",
    "                .load(\"./datasets/titanic.csv\")\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PassengerId: integer (nullable = true)\n",
      " |-- Survived: integer (nullable = true)\n",
      " |-- Pclass: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- SibSp: integer (nullable = true)\n",
      " |-- Parch: integer (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Cabin: string (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res4: Array[org.apache.spark.sql.Row] = Array([1,0,3,Braund, Mr. Owen Harris,male,22.0,1,0,A/5 21171,7.25,null,S])\r\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+------+----+-----+-----+-------+--------+\n",
      "|label|Pclass|   Sex| Age|SibSp|Parch|   Fare|Embarked|\n",
      "+-----+------+------+----+-----+-----+-------+--------+\n",
      "|    0|     3|  male|22.0|    1|    0|   7.25|       S|\n",
      "|    1|     1|female|38.0|    1|    0|71.2833|       C|\n",
      "|    1|     3|female|26.0|    0|    0|  7.925|       S|\n",
      "|    1|     1|female|35.0|    1|    0|   53.1|       S|\n",
      "|    0|     3|  male|35.0|    0|    0|   8.05|       S|\n",
      "|    0|     3|  male|null|    0|    0| 8.4583|       Q|\n",
      "|    0|     1|  male|54.0|    0|    0|51.8625|       S|\n",
      "|    0|     3|  male| 2.0|    3|    1| 21.075|       S|\n",
      "|    1|     3|female|27.0|    0|    2|11.1333|       S|\n",
      "|    1|     2|female|14.0|    1|    0|30.0708|       C|\n",
      "|    1|     3|female| 4.0|    1|    1|   16.7|       S|\n",
      "|    1|     1|female|58.0|    0|    0|  26.55|       S|\n",
      "|    0|     3|  male|20.0|    0|    0|   8.05|       S|\n",
      "|    0|     3|  male|39.0|    1|    5| 31.275|       S|\n",
      "|    0|     3|female|14.0|    0|    0| 7.8542|       S|\n",
      "|    1|     2|female|55.0|    0|    0|   16.0|       S|\n",
      "|    0|     3|  male| 2.0|    4|    1| 29.125|       Q|\n",
      "|    1|     2|  male|null|    0|    0|   13.0|       S|\n",
      "|    0|     3|female|31.0|    1|    0|   18.0|       S|\n",
      "|    1|     3|female|null|    0|    0|  7.225|       C|\n",
      "+-----+------+------+----+-----+-----+-------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [label: int, Pclass: int ... 6 more fields]\r\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = data.select(data(\"Survived\").as(\"label\"),\n",
    "                     $\"Pclass\", $\"Sex\", $\"Age\", $\"SibSp\", $\"Parch\", $\"Fare\", $\"Embarked\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "final_df: org.apache.spark.sql.DataFrame = [label: int, Pclass: int ... 6 more fields]\r\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val final_df = df.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.{VectorAssembler, StringIndexer, VectorIndexer, OneHotEncoder}\r\n",
       "import org.apache.spark.ml.linalg.Vectors\r\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.{VectorAssembler, StringIndexer, VectorIndexer, OneHotEncoder}\n",
    "import org.apache.spark.ml.linalg.Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "genderIndexer: org.apache.spark.ml.feature.StringIndexer = strIdx_916515b42705\r\n",
       "embarkIndexer: org.apache.spark.ml.feature.StringIndexer = strIdx_501bc4cfd167\r\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val genderIndexer = new StringIndexer()\n",
    "                    .setInputCol(\"Sex\")\n",
    "                    .setOutputCol(\"SexIndex\")\n",
    "val embarkIndexer = new StringIndexer()\n",
    "                    .setInputCol(\"Embarked\")\n",
    "                    .setOutputCol(\"EmbarkIndex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "genderEncoder: org.apache.spark.ml.feature.OneHotEncoder = oneHotEncoder_d9bdf150cd58\r\n",
       "embarkEncoder: org.apache.spark.ml.feature.OneHotEncoder = oneHotEncoder_2989a5eb4db6\r\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val genderEncoder = new OneHotEncoder()\n",
    "                    .setInputCol(\"SexIndex\")\n",
    "                    .setOutputCol(\"SexVec\")\n",
    "val embarkEncoder = new OneHotEncoder()\n",
    "                    .setInputCol(\"EmbarkIndex\")\n",
    "                    .setOutputCol(\"EmbarkVec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "assembler: org.apache.spark.ml.feature.VectorAssembler = VectorAssembler: uid=vecAssembler_38bf00bc7e8b, handleInvalid=error, numInputCols=7\r\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val assembler = new VectorAssembler()\n",
    "                .setInputCols(Array(\"Pclass\", \"SexVec\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"EmbarkVec\"))\n",
    "                .setOutputCol(\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [label: int, Pclass: int ... 6 more fields]\r\n",
       "test: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [label: int, Pclass: int ... 6 more fields]\r\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val Array(train, test) = final_df.randomSplit(Array(0.7, 0.3), seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lr: org.apache.spark.ml.classification.LogisticRegression = logreg_3d4c5a01dfba\r\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lr = new LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.Pipeline\r\n",
       "pipeline: org.apache.spark.ml.Pipeline = pipeline_c5a0454cf328\r\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.Pipeline\n",
    "\n",
    "val pipeline = new Pipeline()\n",
    "                .setStages(Array(genderIndexer, embarkIndexer, genderEncoder, embarkEncoder, assembler, lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model: org.apache.spark.ml.PipelineModel = pipeline_c5a0454cf328\r\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val model = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "results: org.apache.spark.sql.DataFrame = [label: int, Pclass: int ... 14 more fields]\r\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val results = model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.mllib.evaluation.MulticlassMetrics\r\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.mllib.evaluation.MulticlassMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predictionAndLabels: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[198] at rdd at <console>:38\r\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val predictionAndLabels = results.select($\"prediction\", $\"label\")\n",
    "                            .as[(Double, Double)].rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "metrics: org.apache.spark.mllib.evaluation.MulticlassMetrics = org.apache.spark.mllib.evaluation.MulticlassMetrics@6093dfb6\r\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val metrics = new MulticlassMetrics(predictionAndLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res7: org.apache.spark.mllib.linalg.Matrix =\r\n",
       "83.0  22.0\r\n",
       "20.0  63.0\r\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusionMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res8: Double = 0.776595744680851\r\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
